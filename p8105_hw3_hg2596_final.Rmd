---
title: "p8105_hw3_hg2596"
output: github_document
date: "2022-10-18"
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggridges)
library(patchwork)

library(p8105.datasets)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

### Problem 0

This solution focuses on a reproducible report containing code and text necessary for Problems 1-3, and is organized as an R Project. This was not prepared as a GitHub repo; examples for repository structure and git commits should be familiar from other elements of the course.

Throughout, we use appropriate text to describe our code and results, and use clear styling to ensure code is readable. 

### Problem 1

#### Read in the data

```{r}
data("instacart")

instacart = 
  instacart %>% 
  as_tibble(instacart)
```

#### Answer questions about the data

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns, with each row resprenting a single product from an instacart order. Variables include identifiers for user, order, and product; the order in which each product was added to the cart. There are several order-level variables, describing the day and time of the order, and number of days since prior order. Then there are several item-specific variables, describing the product name (e.g. Yogurt, Avocado), department (e.g. dairy and eggs, produce), and aisle (e.g. yogurt, fresh fruits), and whether the item has been ordered by this user in the past. In total, there are `r instacart %>% select(product_id) %>% distinct %>% count` products found in `r instacart %>% select(user_id, order_id) %>% distinct %>% count` orders from `r instacart %>% select(user_id) %>% distinct %>% count` distinct users.

Below is a table summarizing the number of items ordered from aisle. In total, there are 134 aisles, with fresh vegetables and fresh fruits holding the most items ordered by far.

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

Next is a plot that shows the number of items ordered in each aisle. Here, aisles are ordered by ascending number of items.

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))

```

Our next table shows the three most popular items in aisles `baking ingredients`, `dog food care`, and `packaged vegetables fruits`, and includes the number of times each item is ordered in your table.

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```

Finally is a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week. This table has been formatted in an untidy manner for human readers. Pink Lady Apples are generally purchased slightly earlier in the day than Coffee Ice Cream, with the exception of day 5.

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

# Problem 2 

```{r}
accel_1 = read_csv("./data/accel_data.csv")%>%
  janitor::clean_names() 
  
```

```{r}

accel_2 =  pivot_longer(
    accel_1, 
    activity_1:activity_1440,
    names_to = "mins",  
    names_prefix = "activity_",
    values_to = "count"
    ) %>%
  mutate(Week_identify = ifelse(day%in%c("Saturday", "Sunday"), 2, 1)) %>%
  mutate(day = fct_relevel(day,"Monday","Tuesday","Wednesday", "Thursday", "Friday","Saturday", "Sunday"))
```
# Part 1

Originally had over 1440 columns, the following variables were **week** (1-5),**day_ID** (just a count), **day** (Mon-Sun), **activity_1**, **activity_2**...and so on for following the patient for 24 hours. 

Now, after pivoting longer, we are able to put the counts is **counts** making it one variable,and made a new variable **mins** to keep track which count came from which minute of the week.  

There are now `r nrow(accel_2)` observations and `r ncol(accel_2) `  columns. 

```{r}
accel_3 = 
accel_2 %>%
  mutate(day = as.factor(day))%>%
  group_by(week,day) %>%
  summarize(total_activity = sum(count))
```

```{r}
accel_4 = accel_3 %>%
  pivot_wider(
  names_from = "day", 
  values_from = "total_activity")

accel_4 
```

# Part 2

Just eyeballing the table, it looks like the for week 4 and week 5 there is a **big drop** in counts on Saturday. On average, Mondays on average higher counts. Overall, though its difficult to see very apparent trends just based on the table. 

```{r}
ggplot(accel_2, aes(x = mins, y = count, color = day)) + 
  geom_line() +
   labs(
    title = "Scatter Plot of Count over 24 hours",
    x = "Minutes(mins)",
    y = "Count",
  )
```

# Part 3 

There is a lot of data, so it is very difficult to distinguish patterns.However, some of the trends is that that count on **average is below 2500**. Another trend is there seems to be peak minutes when the count goes above the average count, its difficult to know exactly what exact minutes but it seems their are **two high peaks** during the morning and later on toward the latter portion of the 24 hour period. 

# Problem 3

```{r}
data("ny_noaa")
```

```{r}
noaa_2 = ny_noaa %>% 
  janitor::clean_names() %>%
  separate(date,c("A", "B", "C")) %>%
  select("B","C","A", everything()) %>%
  rename(month = "B", day = "C", year = "A") %>%
  mutate(month = month.abb[as.numeric(month)]) %>%
  mutate(tmax = as.integer(tmax)) %>%
  mutate(tmin = as.integer(tmin)) %>%
  select(id, everything()) 

```

```{r}
noaa_3= 
  noaa_2 %>% 
  mutate(tmax_new = tmax/10) %>%
  mutate(tmin_new = tmin/10) %>%
  mutate(prcp_new = prcp/10) %>% 
  mutate_if(is.double,as.integer)%>%
  select(-tmax,-tmin,-prcp)
  

skimr::skim(noaa_3)
```

```{r}
noaa_3 %>% 
  count(snow, name = "n_obs", sort = TRUE)
```

# Part 1

After cleaning,  **ID**, **month**, **day**, **year**,**snow**, **snwd**, 
**tmax_new**, **tmin_new** and **prcp_new**. With `r nrow(noaa_3) ` 
observations and `r ncol(noaa_3) ` columns. 

Another cleaning step was dividing any variable that was measured in **tenth of a unit** by **10** so that the units were a little easier to read and understand. Leaving the units to be in Celsius and mm. 

There are unfortunately a lot of missing data across all variables especially **tmax** and **tmin** but when looking at the variables means they seem accurate to what one would expect levels of temperature minimum and max temperatures (similarly in other variables) so I think even despite the missing observations, the data will provide good information. 

Most common value in the variable **snow** is '0'. This does logically make sense because snow only really occurs 3 months out of the 12 months in a year, the largest value being no snow day makes sense in context of a whole year. 


```{r}
noaa_3 %>%
  mutate(tmax_new = as.numeric(tmax_new)) %>%
  group_by(id, month) %>%
  filter(month == "Jan"| month == "Jul") %>%
  summarize(avg_tmax = mean(tmax_new)) %>%
  ggplot(aes(x = id, y = avg_tmax)) + geom_point(alpha = 0.8) +
  facet_grid(. ~ month) +
  labs(
    title = "Scatter Plot of Average Max Temperture in July and January", 
    x = "ID",
    y = "Average Max Temperature (C)", 
  )

```

# Part 2

The main take away would be that in January we would expect average max temperatures to be low compared to July, which the plot clearly shows. As for as outliers, it is difficult to see outliers but there some to be some locations that have higher temperatures then one might expect in January. Similarly, there are some locations that have lower temperatures in July then one might expect. 

# Part 3

```{r}
graph = 
  noaa_3 %>% 
  mutate(year = as.factor(year)) %>%
  ggplot(aes(x = tmax_new, y = tmin_new)) + 
  geom_hex() +
labs(
    title = "Hexplot of Tmax vs Tmin",
    x = "Maxiumum Temperatue (C)",
    y = "Minimum Temperature (C)"
  )
```


```{r}

density = 
  noaa_2 %>%
  mutate(year = as.factor(year)) %>%
  filter(snow < 100) %>%
  filter(snow >= 0) %>%
  ggplot(aes(x = snow , y = year)) + geom_density_ridges(alpha = 0.5) +
labs(
    title = "Distribution of Snow with Density Plot",
    x = "Snow (mm)", 
    y = "Year"
  ) 
    

```

```{r}
(graph + density)
```



















































